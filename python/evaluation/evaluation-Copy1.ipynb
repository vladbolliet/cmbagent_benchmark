{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a7721d-67c5-4bee-af2e-dcea15401e47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Optional \n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "import cmbagent\n",
    "import re # for regex\n",
    "import ast  # safe way to parse Python literals like lists, dicts\n",
    "import pandas as pd\n",
    "from tabulate import tabulate #for printing pretty tables in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8021d64-21a5-4b90-b437-2ab83b975542",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# get openai API KEY\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2470999e-1615-48fd-9022-f37dd9bdb0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e00175a-b80d-4e51-bb13-f3469dcc22fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load all problems from a JSON file into a python dict\n",
    "\n",
    "def load_problems(json_path: str) -> dict: \n",
    "    with open(json_path, 'r') as f:\n",
    "        problems = json.load(f)\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e65961-797e-4f64-88f3-dba3ce2a0e38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load_test_cases_for_one_problem as a list of tuples of lists\n",
    "\n",
    "def load_test_cases_for_one_problem(problem_id: str, tests_dir: str) -> list | None:\n",
    "    test_cases = []\n",
    "    i = 1\n",
    "    while True:\n",
    "        input_file = os.path.join(tests_dir, problem_id, f\"I.{i}\")\n",
    "        output_file = os.path.join(tests_dir, problem_id, f\"O.{i}\")\n",
    "\n",
    "        if not os.path.exists(input_file) or not os.path.exists(output_file):\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            with open(input_file, \"r\") as f_in:\n",
    "                input_list = list(map(int, f_in.read().strip().split()))\n",
    "\n",
    "            with open(output_file, \"r\") as f_out:\n",
    "                output_list = list(map(int, f_out.read().strip().split()))\n",
    "\n",
    "            test_cases.append((input_list, output_list))\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"⚠️ Skipping problem {problem_id} due to non-numeric data in test case {i}: {e}\")\n",
    "            return None\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return test_cases if test_cases else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcd1ea2-7aa6-4597-8048-754638e89c7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_cmbagent_benchmark_summary(results_summary: dict) -> None:\n",
    "    from tabulate import tabulate\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"\\n============ BENCHMARK SUMMARY FOR CMBAGENT FOR ALL PROBLEMS ==============\")\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    total_problems = len(results_summary)\n",
    "    all_costs = []\n",
    "\n",
    "    for problem_id, stats in results_summary.items():\n",
    "        print(f\"{problem_id}:\\n\"\n",
    "              f\"  Total test cases: {stats['total']}\\n\"\n",
    "              f\"  Correctly solved: {stats['correct']}\\n\"\n",
    "              f\"  Accuracy: {stats['accuracy']:.2f}%\\n\")\n",
    "        total_accuracy += stats['accuracy']\n",
    "\n",
    "        cost_df = stats.get(\"cost_dataframe\")\n",
    "        if isinstance(cost_df, pd.DataFrame) and not cost_df.empty:\n",
    "            all_costs.append(cost_df[cost_df[\"Agent\"] != \"Total\"])  # only agent-level rows\n",
    "\n",
    "    average_accuracy = (total_accuracy / total_problems) if total_problems > 0 else 0\n",
    "\n",
    "    print(\"============ CONCLUSION ===============\")\n",
    "    print(f\"Average accuracy over all problems: {average_accuracy:.2f}%\")\n",
    "    print(\"=======================================\\n\")\n",
    "\n",
    "    # Final benchmark-wide cost aggregation\n",
    "    if all_costs:\n",
    "        benchmark_cost_df = pd.concat(all_costs, ignore_index=True)\n",
    "        benchmark_cost_df = benchmark_cost_df.groupby(\"Agent\", as_index=False).sum(numeric_only=True)\n",
    "        total_row = benchmark_cost_df.drop(columns=[\"Agent\"]).sum(numeric_only=True)\n",
    "        total_row[\"Agent\"] = \"Total\"\n",
    "        benchmark_cost_df = pd.concat([benchmark_cost_df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "        print(\"======================= FINAL BENCHMARK COST SUMMARY =======================\")\n",
    "        print(tabulate(benchmark_cost_df, headers=\"keys\", tablefmt=\"github\"))\n",
    "        print(\"============================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ab05c5-9c9c-4ef1-a151-b3b6ae15ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for evaluating cmbagent with agent = 'engineer' (executes code by himself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d56b45-2edd-4ab5-a1e6-41db4b42b79e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# find result found by cmbagent through execution with regex\n",
    "\n",
    "def find_result_in_cmbagent_string(cmbagent_answer: dict) -> list[int] | None:\n",
    "    last_execution_output_message = None\n",
    "\n",
    "    for message in reversed(cmbagent_answer['chat_history']):\n",
    "        content = message.get('content', '')\n",
    "        if \"Execution output:\" in content:\n",
    "            last_execution_output_message = content\n",
    "            break\n",
    "    \n",
    "    if last_execution_output_message:\n",
    "        match = re.search(r'Execution output:\\s*(.*)', last_execution_output_message)\n",
    "        if match:\n",
    "            result_str = match.group(1).strip()\n",
    "            try:\n",
    "                # Safely parse the string representation of a Python literal (like a list)\n",
    "                result_list = ast.literal_eval(result_str)\n",
    "                # Optional: verify it's a list of ints\n",
    "                if isinstance(result_list, list) and all(isinstance(x, int) for x in result_list):\n",
    "                    return result_list\n",
    "                else:\n",
    "                    print(\"Parsed result is not a list of ints:\", result_list)\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(\"Error parsing output string:\", e)\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Pattern found but couldn't parse output.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No execution output found in chat history.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c0f3f9-6639-4b60-830e-9a10a9e7fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for evaluating cmbagent with agent = 'researcher' (execute code locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84af9a09-3c76-43da-92e5-ea6777cfdad3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_code(model_answer: dict) -> str | None:\n",
    "    content = model_answer[\"chat_history\"][2][\"content\"]\n",
    "    matches = re.findall(r\"<code>(.*?)</code>\", content, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[-1].strip()  # Return the last <code>...</code> block\n",
    "    print(\"⚠️ No <code>...</code> block found.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187c18ea-56c1-4cfd-b60c-859d840d77b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_python_code_locally_for_one_test_case(code_str: str, input_data: list[int]) -> list[int]:\n",
    "    exec_locals = {}\n",
    "\n",
    "    # run code \n",
    "    \n",
    "    try:\n",
    "        exec(code_str, {}, exec_locals)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Code execution failed: {e}\")\n",
    "\n",
    "    # get main function\n",
    "    \n",
    "    main_func = exec_locals.get(\"main_function\")\n",
    "    \n",
    "    if not main_func:\n",
    "        raise RuntimeError(\"No function named 'main_function' found in code.\")\n",
    "\n",
    "    # run example on function\n",
    "    \n",
    "    try:\n",
    "        result = main_func(input_data)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error when calling main_function: {e}\")\n",
    "\n",
    "    # return result (it is an array)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c10d8624-591f-44f5-8b67-52c6727abbfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# main benchamrk function for cmbagent\n",
    "\n",
    "def run_benchmark_on_cmbagent(problems: dict, problem_dir: str, cmbagent_model: str, agent: str) -> dict:\n",
    "\n",
    "    if agent not in {\"engineer\", \"researcher\"}:\n",
    "        raise ValueError(\"Agent must be 'engineer' or 'researcher'\")\n",
    "\n",
    "    results_summary = {}\n",
    "    all_problem_costs = []  # collect all cost DataFrames here\n",
    "    skipped_problems = []\n",
    "\n",
    "    total_problems = len(problems)\n",
    "    c = 0\n",
    "    \n",
    "    for problem_index, (problem_id, problem) in enumerate(problems.items(), start=1):\n",
    "        d = 0\n",
    "        \n",
    "        print(\"===============================================================\")\n",
    "        print(f\"\\t\\tEvaluating CMBAgent on problem {problem_index}/{total_problems}: {problem_id}\")\n",
    "        print(\"===============================================================\")\n",
    "\n",
    "        test_cases = load_test_cases_for_one_problem(problem_id, problem_dir)\n",
    "        \n",
    "        if not test_cases:\n",
    "            print(f\"No test cases found for {problem_id}\")\n",
    "            skipped_problems.append(problem_id)\n",
    "            continue\n",
    "\n",
    "        example_input, example_output = test_cases[0]\n",
    "\n",
    "        if agent == \"engineer\":\n",
    "            prompt = (\n",
    "                f\"Task: {problem['description']}\\n\"\n",
    "                f\"Example:\\n\"\n",
    "                f\"Input: {example_input}\\n\"\n",
    "                f\"Expected Output: {example_output}\\n\"\n",
    "                f\"Deliver the result strictly as a Python list:\\n\"\n",
    "                f\"\\t- If the result is a single integer n, return it as [n]\\n\"\n",
    "                f\"\\t- If the result is a list, return it as [a, b, c, ...]\\n\"\n",
    "                f\"Do not include any extra text, explanation, or formatting.\"\n",
    "            )\n",
    "\n",
    "        else:  # researcher\n",
    "            prompt = (\n",
    "                f\"Task:\\nWrite a function which solves the following problem: {problem['description']}\\n\"\n",
    "                f\"Always name the primary function main_function, even if it requires helper functions\\n\"\n",
    "                f\"Example:\\n\"\n",
    "                f\"Function Input: {example_input}\\n\"\n",
    "                f\"Expected Function Output: {example_output}\\n\"\n",
    "                f\"The function has to deliver the result strictly as a Python list:\\n\"\n",
    "                f\"\\t- If the result is a single integer n, return it as [n]\\n\"\n",
    "                f\"\\t- If the result is a list, return it as [a, b, c, ...]\\n\"\n",
    "                f\"Do not include any extra text, explanation, or formatting.\"\n",
    "            )\n",
    "\n",
    "            model_answer = cmbagent.one_shot(\n",
    "                prompt,\n",
    "                max_rounds=10,\n",
    "                agent='researcher',\n",
    "                engineer_model=cmbagent_model,\n",
    "            )\n",
    "\n",
    "            code_str = extract_code(model_answer)\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        cost_dfs = []\n",
    "\n",
    "        for i, (input_list, expected_output) in enumerate(test_cases[1:], start=1):\n",
    "\n",
    "            if agent == \"engineer\":\n",
    "\n",
    "                if i > 1:\n",
    "                    print(\"========================================================\")\n",
    "                    print(f\"\\t\\tNEXT TEST CASE ({i}/{len(test_cases) - 1}, on problem {problem_index}/{total_problems})\")\n",
    "                    print(\"========================================================\\n\")\n",
    "\n",
    "                test_prompt = (\n",
    "                    prompt +\n",
    "                    f\"\\nFind the answer for the following input:\\n{input_list}\\nOutput: ?\"\n",
    "                )\n",
    "\n",
    "                model_answer = cmbagent.one_shot(\n",
    "                    test_prompt,\n",
    "                    max_rounds=10,\n",
    "                    agent='engineer',\n",
    "                    engineer_model=cmbagent_model,\n",
    "                )\n",
    "\n",
    "                parsed_answer = find_result_in_cmbagent_string(model_answer)\n",
    "\n",
    "                if parsed_answer == expected_output:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "                cost_df = model_answer[\"final_context\"].data.get(\"cost_dataframe\", pd.DataFrame())\n",
    "                if not cost_df.empty:\n",
    "                    cost_df = cost_df[cost_df[\"Agent\"] != \"Total\"]\n",
    "                    cost_dfs.append(cost_df)\n",
    "\n",
    "            else:  # researcher agent\n",
    "\n",
    "                print(f\"Executing code for TEST CASE {i}/{len(test_cases) - 1}, on problem {problem_index}/{total_problems}\\n\")\n",
    "\n",
    "                result_from_code = run_python_code_locally_for_one_test_case(code_str, input_list)\n",
    "\n",
    "                if not (isinstance(result_from_code, list) and all(isinstance(item, int) for item in result_from_code)):\n",
    "                    print(f\"Result for TEST CASE ({i}/{len(test_cases) - 1}, on problem {problem_index}/{total_problems} is NOT a list\")\n",
    "\n",
    "                if result_from_code == expected_output:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "                # *** Assuming researcher cost data is stored in model_answer's final_context (you may need to adapt this) ***\n",
    "                if i == 1:\n",
    "                    # Example: try to extract cost data from researcher response once per problem (you can change this)\n",
    "                    researcher_cost_df = model_answer[\"final_context\"].data.get(\"cost_dataframe\", pd.DataFrame())\n",
    "                    if not researcher_cost_df.empty:\n",
    "                        researcher_cost_df = researcher_cost_df[researcher_cost_df[\"Agent\"] != \"Total\"]\n",
    "                        cost_dfs.append(researcher_cost_df)\n",
    "\n",
    "            d += 1\n",
    "            if d == 3:\n",
    "                break\n",
    "\n",
    "        accuracy = (correct / total * 100) if total > 0 else 0\n",
    "\n",
    "        # Aggregate cost for this problem\n",
    "\n",
    "        if cost_dfs:\n",
    "            problem_cost_df = pd.concat(cost_dfs, ignore_index=True)\n",
    "            problem_cost_df = problem_cost_df.groupby(\"Agent\", as_index=False).sum(numeric_only=True)\n",
    "            total_row = problem_cost_df.drop(columns=[\"Agent\"]).sum(numeric_only=True)\n",
    "            total_row[\"Agent\"] = \"Total\"\n",
    "            problem_cost_df = pd.concat([problem_cost_df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "            all_problem_costs.append(problem_cost_df[problem_cost_df[\"Agent\"] != \"Total\"])\n",
    "        else:\n",
    "            problem_cost_df = pd.DataFrame()\n",
    "\n",
    "        results_summary[problem_id] = {\n",
    "            \"total\": total,\n",
    "            \"correct\": correct,\n",
    "            \"accuracy\": accuracy,\n",
    "            #\"cost_dataframe\": problem_cost_df  # optional per problem cost\n",
    "        }\n",
    "\n",
    "        print(f\"\\n =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM {problem_id} ============\\n\")\n",
    "        print(f\"Total test cases: {total}\")\n",
    "        print(f\"Correctly guessed test_cases: {correct}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        # Optionally, print problem cost breakdown\n",
    "        # print(tabulate(problem_cost_df, headers='keys', tablefmt='github'))\n",
    "\n",
    "        c += 1\n",
    "        if c == 3:\n",
    "            break\n",
    "\n",
    "    print(\"⚠️ Skipped problems due to non-numeric data:\")\n",
    "    for pid in skipped_problems:\n",
    "        print(f\"- {pid}\")\n",
    "\n",
    "    # Aggregate total cost across all problems\n",
    "    if all_problem_costs:\n",
    "        total_cost_df = pd.concat(all_problem_costs, ignore_index=True)\n",
    "        total_cost_df = total_cost_df.groupby(\"Agent\", as_index=False).sum(numeric_only=True)\n",
    "        total_row = total_cost_df.drop(columns=[\"Agent\"]).sum(numeric_only=True)\n",
    "        total_row[\"Agent\"] = \"Total\"\n",
    "        total_cost_df = pd.concat([total_cost_df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "        print(\"\\n=== Total aggregated cost across all problems ===\")\n",
    "        print(tabulate(total_cost_df, headers=\"keys\", tablefmt=\"github\"))\n",
    "    else:\n",
    "        print(\"No cost data available to summarize.\")\n",
    "\n",
    "    return results_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ccba76f-f138-46f4-8f05-eb68bc3f131f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# main wrapper function (this one is called by user and 'does all the work')\n",
    "\n",
    "def run_benchmark(\n",
    "    problem_json: str,\n",
    "    problem_dir: str, \n",
    "    eval_cmbagent: bool = True, \n",
    "    cmbagent_model: Optional[str] = None,\n",
    "    eval_normal_llm: bool = False, \n",
    "    llm_model: Optional[str] = None\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Run benchmark evaluations on problems located in problem_dir.\n",
    "    \n",
    "    Args:\n",
    "        problem_dir (str): Path to the directory containing problems and test data.\n",
    "        eval_cmbagent (bool): Whether to evaluate cmbagent model.\n",
    "        eval_normal_llm (bool): Whether to evaluate a normal LLM.\n",
    "        llm_model (Optional[str]): The normal LLM model name (required if eval_normal_llm is True).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate arguments\n",
    "\n",
    "    if not eval_cmbagent and not eval_normal_llm:\n",
    "        raise ValueError(\"At least one of eval_cmbagent or eval_normal_llm must be True\")\n",
    "    if eval_normal_llm and not llm_model:\n",
    "        raise ValueError(\"llm_model must be provided if eval_normal_llm is True\")\n",
    "    if not problem_json or not problem_dir:\n",
    "        raise ValueError(\"Both problem_json and problem_dir must be specified\")\n",
    "    if not os.path.exists(problem_json):\n",
    "        raise FileNotFoundError(f\"Problem JSON file not found: {problem_json}\")\n",
    "    if not os.path.exists(problem_dir):\n",
    "        raise FileNotFoundError(f\"Problem directory not found: {problem_dir}\")\n",
    "    if eval_cmbagent and not cmbagent_model:\n",
    "        raise ValueError(\"cmbagent_model must be provided if eval_cmbagent is True\")\n",
    "    \n",
    "    # Load problems\n",
    "\n",
    "    problems = load_problems(problem_json)\n",
    "    \n",
    "    # evaluate cmbagent on problems\n",
    "\n",
    "    if eval_cmbagent: \n",
    "        results = run_benchmark_on_cmbagent(problems, problem_dir, cmbagent_model, \"researcher\", )\n",
    "        print_cmbagent_benchmark_summary(results)\n",
    "        \n",
    "    \n",
    "    # evaluate the normal LLM on problems\n",
    "\n",
    "    if eval_normal_llm:\n",
    "        run_benchmark_on_normal_llm(problems, problem_dir, llm_model)\n",
    "    \n",
    "    # Print or save benchmark results\n",
    "\n",
    "    print(\"Benchmark evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ce40c71-d332-4f6b-8b0e-06e388d765b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "148e851b-58dc-4df3-8932-6b48bebdba53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 1/4: 0001_easy_addition\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: Given two integers A and B, output their sum.\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [1, 2]\n",
      "Expected Function Output: [3]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.00124            521                 25           546\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    A, B = inputs\n",
      "    return [A + B]\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00152            335                262           597\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: main_function_notes.md -->\n",
      "Below is the main function code snippet:\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    A, B = inputs\n",
      "    return [A + B]\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/main_function_notes.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00209            887                 39           926\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00084            418                  1           419\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00208600 |           887 |                39 |          926 |\n",
      "| researcher                    | $0.00124200 |           521 |                25 |          546 |\n",
      "| researcher response formatter | $0.00152130 |           335 |               262 |          597 |\n",
      "| terminator                    | $0.00084400 |           418 |                 1 |          419 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.00569330 |          2161 |               327 |         2488 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250608_045312.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250608_045312.json\n",
      "\n",
      "Task took 10.8994 seconds\n",
      "Executing code for TEST CASE 1/2, on problem 1/4\n",
      "\n",
      "Executing code for TEST CASE 2/2, on problem 1/4\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 0001_easy_addition ============\n",
      "\n",
      "Total test cases: 2\n",
      "Correctly guessed test_cases: 2\n",
      "Accuracy: 100.00%\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 2/4: 0002_easy_max\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: Given two integers, output the maximum.\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [5, 8]\n",
      "Expected Function Output: [8]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.00121            515                 23           538\n",
      "<code>\n",
      "def main_function(lst):\n",
      "    return [max(lst[0], lst[1])]\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00174            333                313           646\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: python_code.md -->\n",
      "<code>\n",
      "def main_function(lst):\n",
      "    return [max(lst[0], lst[1])]\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/python_code.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00205            870                 39           909\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00081            401                  1           402\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00205200 |           870 |                39 |          909 |\n",
      "| researcher                    | $0.00121400 |           515 |                23 |          538 |\n",
      "| researcher response formatter | $0.00174350 |           333 |               313 |          646 |\n",
      "| terminator                    | $0.00081000 |           401 |                 1 |          402 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.00581950 |          2119 |               376 |         2495 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250608_045323.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250608_045323.json\n",
      "\n",
      "Task took 9.3211 seconds\n",
      "Executing code for TEST CASE 1/1, on problem 2/4\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 0002_easy_max ============\n",
      "\n",
      "Total test cases: 1\n",
      "Correctly guessed test_cases: 1\n",
      "Accuracy: 100.00%\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 3/4: 0003_easy_even_check\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: Given an integer, output '1' if it is even, otherwise output '0'.\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [4]\n",
      "Expected Function Output: [1]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.00131            529                 31           560\n",
      "<code>\n",
      "def main_function(x):\n",
      "    return [1] if x[0] % 2 == 0 else [0]\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00236            341                450           791\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: even_odd_function.md -->\n",
      "<code>\n",
      "def main_function(x):\n",
      "    return [1] if x[0] % 2 == 0 else [0]\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/even_odd_function.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00215            898                 44           942\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00088            434                  1           435\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00214800 |           898 |                44 |          942 |\n",
      "| researcher                    | $0.00130600 |           529 |                31 |          560 |\n",
      "| researcher response formatter | $0.00235510 |           341 |               450 |          791 |\n",
      "| terminator                    | $0.00087600 |           434 |                 1 |          435 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.00668510 |          2202 |               526 |         2728 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250608_045337.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250608_045337.json\n",
      "\n",
      "Task took 11.4911 seconds\n",
      "Executing code for TEST CASE 1/1, on problem 3/4\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 0003_easy_even_check ============\n",
      "\n",
      "Total test cases: 1\n",
      "Correctly guessed test_cases: 1\n",
      "Accuracy: 100.00%\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 4/4: 0004_easy_diff\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: Given two integers, output the absolute difference.\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [9, 4]\n",
      "Expected Function Output: [5]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.00125            517                 27           544\n",
      "<code>\n",
      "def main_function(input_list):\n",
      "    return [abs(input_list[0] - input_list[1])]\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00211            337                395           732\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: main_function.md -->\n",
      "Below is the implementation of the main_function:\n",
      "\n",
      "<code>\n",
      "def main_function(input_list):\n",
      "    return [abs(input_list[0] - input_list[1])]\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/main_function.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00209            888                 39           927\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00085            419                  1           420\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00208800 |           888 |                39 |          927 |\n",
      "| researcher                    | $0.00125000 |           517 |                27 |          544 |\n",
      "| researcher response formatter | $0.00210870 |           337 |               395 |          732 |\n",
      "| terminator                    | $0.00084600 |           419 |                 1 |          420 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.00629270 |          2161 |               462 |         2623 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250608_045350.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250608_045350.json\n",
      "\n",
      "Task took 9.3962 seconds\n",
      "Executing code for TEST CASE 1/1, on problem 4/4\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 0004_easy_diff ============\n",
      "\n",
      "Total test cases: 1\n",
      "Correctly guessed test_cases: 1\n",
      "Accuracy: 100.00%\n",
      "⚠️ Skipped problems due to non-numeric data:\n",
      "\n",
      "=== Total aggregated cost across all problems ===\n",
      "|    | Agent                         |   Cost ($) |   Prompt Tokens |   Completion Tokens |   Total Tokens |\n",
      "|----|-------------------------------|------------|-----------------|---------------------|----------------|\n",
      "|  0 | control                       |  0.008374  |            3543 |                 161 |           3704 |\n",
      "|  1 | researcher                    |  0.005012  |            2082 |                 106 |           2188 |\n",
      "|  2 | researcher response formatter |  0.0077286 |            1346 |                1420 |           2766 |\n",
      "|  3 | terminator                    |  0.003376  |            1672 |                   4 |           1676 |\n",
      "|  4 | Total                         |  0.0244906 |            8643 |                1691 |          10334 |\n",
      "\n",
      "============ BENCHMARK SUMMARY FOR CMBAGENT FOR ALL PROBLEMS ==============\n",
      "0001_easy_addition:\n",
      "  Total test cases: 2\n",
      "  Correctly solved: 2\n",
      "  Accuracy: 100.00%\n",
      "\n",
      "0002_easy_max:\n",
      "  Total test cases: 1\n",
      "  Correctly solved: 1\n",
      "  Accuracy: 100.00%\n",
      "\n",
      "0003_easy_even_check:\n",
      "  Total test cases: 1\n",
      "  Correctly solved: 1\n",
      "  Accuracy: 100.00%\n",
      "\n",
      "0004_easy_diff:\n",
      "  Total test cases: 1\n",
      "  Correctly solved: 1\n",
      "  Accuracy: 100.00%\n",
      "\n",
      "============ CONCLUSION ===============\n",
      "Average accuracy over all problems: 100.00%\n",
      "=======================================\n",
      "\n",
      "Benchmark evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "# benchmark on 4 extremely easy problems to make sure everything works properly\n",
    "\n",
    "run_benchmark(\n",
    "    problem_json=\"/mnt/p/stage/cmbagent_benchmark/data/clean/easy_custom_samples.json\",\n",
    "    problem_dir=\"/mnt/p/stage/cmbagent_benchmark/data/clean/easy_tests\",\n",
    "    eval_cmbagent=True,\n",
    "    cmbagent_model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b634e0e-de6a-4980-9822-4cc5427d6989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 1/307: 1333_platinum_good_bitstrings\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: \n",
      "For any two positive integers $a$ and $b$, define the function\n",
      "$\\texttt{gen_string}(a,b)$ by the following Python code:\n",
      "\n",
      "\n",
      "def gen_string(a: int, b: int):\n",
      "\tres = \"\"\n",
      "\tia, ib = 0, 0\n",
      "\twhile ia + ib < a + b:\n",
      "\t\tif ia * b <= ib * a:\n",
      "\t\t\tres += '0'\n",
      "\t\t\tia += 1\n",
      "\t\telse:\n",
      "\t\t\tres += '1'\n",
      "\t\t\tib += 1\n",
      "\treturn res\n",
      "\n",
      "Equivalent C++ code:\n",
      "\n",
      "\n",
      "string gen_string(int64_t a, int64_t b) {\n",
      "\tstring res;\n",
      "\tint ia = 0, ib = 0;\n",
      "\twhile (ia + ib < a + b) {\n",
      "\t\tif ((__int128)ia * b <= (__int128)ib * a) {\n",
      "\t\t\tres += '0';\n",
      "\t\t\tia++;\n",
      "\t\t} else {\n",
      "\t\t\tres += '1';\n",
      "\t\t\tib++;\n",
      "\t\t}\n",
      "\t}\n",
      "\treturn res;\n",
      "}\n",
      "\n",
      "$ia$ will equal $a$ and $ib$ will equal $b$ when the loop terminates, so this\n",
      "function returns a  bitstring of length $a+b$ with exactly $a$ zeroes and $b$\n",
      "ones. For example, $\\texttt{gen_string}(4,10)=01110110111011$.\n",
      "\n",
      "Call a bitstring $s$ $\\textbf{good}$ if there exist positive integers $x$ and\n",
      "$y$  such that $s=\\texttt{gen_string}(x,y)$. Given two positive integers $A$ and\n",
      "$B$  ($1\\le A,B\\le 10^{18}$), your job is to compute the number of good prefixes\n",
      "of  $\\texttt{gen_string}(A,B)$. For example, there are $6$ good prefixes of \n",
      "$\\texttt{gen_string}(4,10)$:\n",
      "\n",
      "\n",
      "x = 1 | y = 1 | gen_string(x, y) = 01\n",
      "x = 1 | y = 2 | gen_string(x, y) = 011\n",
      "x = 1 | y = 3 | gen_string(x, y) = 0111\n",
      "x = 2 | y = 5 | gen_string(x, y) = 0111011\n",
      "x = 3 | y = 7 | gen_string(x, y) = 0111011011\n",
      "x = 4 | y = 10 | gen_string(x, y) = 01110110111011\n",
      "\n",
      "INPUT FORMAT (input arrives from the terminal / stdin):\n",
      "The first line contains $T$ ($1\\le T\\le 10$), the number of independent test\n",
      "cases.\n",
      "\n",
      "Each of the next $T$ lines contains two integers $A$ and $B$.\n",
      "\n",
      "OUTPUT FORMAT (print output to the terminal / stdout):\n",
      "The answer for each test case on a new line.\n",
      "\n",
      "SAMPLE INPUT:\n",
      "6\n",
      "1 1\n",
      "3 5\n",
      "4 7\n",
      "8 20\n",
      "4 10\n",
      "27 21\n",
      "SAMPLE OUTPUT: \n",
      "1\n",
      "5\n",
      "7\n",
      "10\n",
      "6\n",
      "13\n",
      "\n",
      "SCORING:\n",
      "Input 2: $A,B\\le 100$Input 3: $A,B\\le 1000$Inputs 4-7: $A,B\\le 10^6$Inputs 8-13: All answers are at most $10^5$.Inputs 14-21: No additional constraints.\n",
      "\n",
      "\n",
      "Problem credits: Benjamin Qi\n",
      "\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [6, 1, 1, 3, 5, 4, 7, 8, 20, 4, 10, 27, 21]\n",
      "Expected Function Output: [1, 5, 7, 10, 6, 13]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.00852           2049                553          2602\n",
      "To solve the problem, it is essential to understand the structure of the bitstring generated by the function `gen_string(a, b)`. The function simulates a process where, at each step, it decides whether to append a '0' or a '1' based on the current counts of zeros (`ia`) and ones (`ib`). The decision is made by comparing `ia * b` and `ib * a`. This process is equivalent to tracing the path from (0,0) to (a,b) on a grid, always moving right (for '0') or up (for '1'), and at each step choosing the direction that keeps the path as close as possible to the straight line from (0,0) to (a,b).\n",
      "\n",
      "A prefix of `gen_string(A, B)` is \"good\" if it is itself the output of `gen_string(x, y)` for some positive integers x and y. The key insight is that the set of good prefixes corresponds to the set of lattice points (x, y) on the path from (0,0) to (A,B) that are visible from the origin, i.e., those for which gcd(x, y) = 1. This is because the path never crosses itself, and the prefix up to (x, y) is exactly `gen_string(x, y)`.\n",
      "\n",
      "Therefore, the number of good prefixes is equal to the number of lattice points (x, y) on the path from (0,0) to (A,B) with gcd(x, y) = 1, excluding (0,0) and including (A,B).\n",
      "\n",
      "The Euclidean algorithm can be used to count these points efficiently. The number of such points is given by the sum over all lattice points (x, y) on the path from (0,0) to (A,B) with gcd(x, y) = 1. This is equivalent to the number of steps in the Euclidean algorithm for (A, B), plus one (for the endpoint).\n",
      "\n",
      "The following code implements this logic:\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    def count_good_prefixes(a, b):\n",
      "        # Counts the number of good prefixes for (a, b)\n",
      "        res = 0\n",
      "        while a and b:\n",
      "            q = a // b\n",
      "            res += q\n",
      "            a, b = b, a % b\n",
      "        return res\n",
      "\n",
      "    T = inputs[0]\n",
      "    out = []\n",
      "    for i in range(T):\n",
      "        A = inputs[1 + 2 * i]\n",
      "        B = inputs[2 + 2 * i]\n",
      "        out.append(count_good_prefixes(A, B))\n",
      "    return out\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00477            863                869          1732\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: gen_string_good_prefixes.md -->\n",
      "To solve the problem, it is essential to understand the structure of the bitstring generated by the function `gen_string(a, b)`. The function simulates a process where, at each step, it decides whether to append a '0' or a '1' based on the current counts of zeros (`ia`) and ones (`ib`). The decision is made by comparing `ia * b` and `ib * a`. This process is equivalent to tracing the path from (0,0) to (a,b) on a grid, always moving right (for '0') or up (for '1'), and at each step choosing the direction that keeps the path as close as possible to the straight line from (0,0) to (a,b).\n",
      "\n",
      "A prefix of `gen_string(A, B)` is \"good\" if it is itself the output of `gen_string(x, y)` for some positive integers x and y. The key insight is that the set of good prefixes corresponds to the set of lattice points (x, y) on the path from (0,0) to (A,B) that are visible from the origin, i.e., those for which gcd(x, y) = 1. This is because the path never crosses itself, and the prefix up to (x, y) is exactly `gen_string(x, y)`.\n",
      "\n",
      "Therefore, the number of good prefixes is equal to the number of lattice points (x, y) on the path from (0,0) to (A,B) with gcd(x, y) = 1, excluding (0,0) and including (A,B).\n",
      "\n",
      "The Euclidean algorithm can be used to count these points efficiently. The number of such points is given by the sum over all lattice points (x, y) on the path from (0,0) to (A,B) with gcd(x, y) = 1. This is equivalent to the number of steps in the Euclidean algorithm for (A, B), plus one (for the endpoint).\n",
      "\n",
      "The following code implements this logic:\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    def count_good_prefixes(a, b):\n",
      "        # Counts the number of good prefixes for (a, b)\n",
      "        res = 0\n",
      "        while a and b:\n",
      "            q = a // b\n",
      "            res += q\n",
      "            a, b = b, a % b\n",
      "        return res\n",
      "\n",
      "    T = inputs[0]\n",
      "    out = []\n",
      "    for i in range(T):\n",
      "        A = inputs[1 + 2 * i]\n",
      "        B = inputs[2 + 2 * i]\n",
      "        out.append(count_good_prefixes(A, B))\n",
      "    return out\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/gen_string_good_prefixes.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00572           2703                 39          2742\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00448           2234                  1          2235\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00571800 |          2703 |                39 |         2742 |\n",
      "| researcher                    | $0.00852200 |          2049 |               553 |         2602 |\n",
      "| researcher response formatter | $0.00477290 |           863 |               869 |         1732 |\n",
      "| terminator                    | $0.00447600 |          2234 |                 1 |         2235 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.02348890 |          7849 |              1462 |         9311 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250609_164039.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250609_164039.json\n",
      "\n",
      "Task took 28.5778 seconds\n",
      "Executing code for TEST CASE 1/20, on problem 1/307\n",
      "\n",
      "Executing code for TEST CASE 2/20, on problem 1/307\n",
      "\n",
      "Executing code for TEST CASE 3/20, on problem 1/307\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 1333_platinum_good_bitstrings ============\n",
      "\n",
      "Total test cases: 3\n",
      "Correctly guessed test_cases: 0\n",
      "Accuracy: 0.00%\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 2/307: 1330_gold_pareidolia\n",
      "===============================================================\n",
      "⚠️ Skipping problem 1330_gold_pareidolia due to non-numeric data in test case 1: invalid literal for int() with base 10: 'besssie'\n",
      "No test cases found for 1330_gold_pareidolia\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 3/307: 1326_silver_milk_sum\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: \n",
      "**Note: The time limit for this problem is 4s, 2x the default.**\n",
      "Farmer John's $N$ cows ($1\\le N\\le 1.5\\cdot 10^5$) have integer milk production\n",
      "values $a_1,\\dots,a_N$.  That is, the $i$th cow produces $a_i$ units of milk per\n",
      "minute, with $0 \\leq a_i \\leq 10^8$.  \n",
      "\n",
      "Each morning, Farmer John starts with all $N$ cows hooked up to his milking\n",
      "machine in the barn.  He is required to unhook them one by one, sending them out\n",
      "for their daily exercise routine.  The first cow he sends out is unhooked after\n",
      "just 1 minute of milking, the second cow he sends out is unhooked after another\n",
      "minute of milking, and so on.  Since the first cow (say, cow $x$) only spends\n",
      "one minute on the milking machine, she contributes only $a_x$ units of total\n",
      "milk.  The second cow (say, cow $y$) spends two total minutes on the milking\n",
      "machine, and therefore contributes $2a_y$ units of total milk.  The third cow\n",
      "(say, cow $z$) contributes $3a_z$ total units, and so on.  Let $T$ represent the\n",
      "maximum possible amount of milk, in total, that Farmer John can collect, if he\n",
      "unhooks his cows in an optimal order.\n",
      "\n",
      "Farmer John is curious how $T$ would be affected if some of the milk production\n",
      "values in his herd were different.  For each of $Q$ queries ($1\\le Q\\le 1.5\\cdot 10^5$),\n",
      "each specified by two integers $i$ and $j$, please calculate what would be the \n",
      "new value of $T$ if $a_i$ were set to $j$ ($0 \\leq j \\leq 10^8$).  Note that\n",
      "each query is considering a temporary potential change independent of all other\n",
      "queries; that is, $a_i$ reverts back to its original value before the next query\n",
      "is considered.\n",
      "\n",
      "INPUT FORMAT (input arrives from the terminal / stdin):\n",
      "The first line contains $N$. \n",
      "\n",
      "The second line contains $a_1\\dots a_N$.\n",
      "\n",
      "The third line contains $Q$.\n",
      "\n",
      "The next $Q$ lines each contain two space-separated integers $i$ and $j$.\n",
      "\n",
      "OUTPUT FORMAT (print output to the terminal / stdout):\n",
      "Please print the value of $T$ for each of the $Q$ queries on separate lines.\n",
      "\n",
      "SAMPLE INPUT:\n",
      "5\n",
      "1 10 4 2 6\n",
      "3\n",
      "2 1\n",
      "2 8\n",
      "4 5\n",
      "SAMPLE OUTPUT: \n",
      "55\n",
      "81\n",
      "98\n",
      "\n",
      "For the first query, $a$ would become $[1,1,4,2,6]$, and\n",
      "$T =\n",
      "1 \\cdot 1 + 2 \\cdot 1 + 3 \\cdot 2 + 4 \\cdot 4 + 5 \\cdot 6 = 55$.\n",
      "\n",
      "For the second query, $a$ would become $[1,8,4,2,6]$, and\n",
      "$T =\n",
      "1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 4 + 4 \\cdot 6 + 5 \\cdot 8 = 81$.\n",
      "\n",
      "For the third query, $a$ would become $[1,10,4,5,6]$, and\n",
      "$T =\n",
      "1 \\cdot 1 + 2 \\cdot 4 + 3 \\cdot 5 + 4 \\cdot 6 + 5 \\cdot 10 = 98$.\n",
      "\n",
      "SCORING:\n",
      "Inputs 2-4: $N,Q\\le 1000$Inputs 5-11: No additional\n",
      "constraints.\n",
      "\n",
      "\n",
      "Problem credits: Benjamin Qi\n",
      "\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [5, 1, 10, 4, 2, 6, 3, 2, 1, 2, 8, 4, 5]\n",
      "Expected Function Output: [55, 81, 98]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.01363           2285               1133          3418\n",
      "To address the problem efficiently, the following reasoning and approach are adopted:\n",
      "\n",
      "### Problem Analysis\n",
      "\n",
      "- The total milk $T$ is maximized by assigning the largest multipliers (i.e., the largest $k$ in $k \\cdot a$) to the largest $a$ values. Thus, the optimal order is to sort $a$ in increasing order and assign the smallest multiplier to the smallest $a$.\n",
      "- For each query, we are to compute the new $T$ if $a_i$ is set to $j$, and then the cows are optimally reordered.\n",
      "\n",
      "### Efficient Approach\n",
      "\n",
      "1. **Initial Sorting and Preprocessing:**\n",
      "   - Sort the array $a$ and keep track of the original indices.\n",
      "   - Compute the initial $T$ using the sorted array.\n",
      "2. **For Each Query:**\n",
      "   - Remove $a_i$ from the sorted array and insert $j$.\n",
      "   - Find the new position of $j$ in the sorted array.\n",
      "   - Compute the new $T$ efficiently by adjusting only the affected positions.\n",
      "   - To avoid re-sorting for each query, use binary search and prefix sums.\n",
      "\n",
      "### Implementation Details\n",
      "\n",
      "- Use a list of tuples to keep track of values and their original indices.\n",
      "- Use prefix sums to quickly compute the sum of $a$ values up to a certain position.\n",
      "- For each query, use binary search to find where $j$ would be inserted.\n",
      "- Adjust the prefix sums and $T$ accordingly.\n",
      "\n",
      "### Code\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    import bisect\n",
      "\n",
      "    N = inputs[0]\n",
      "    a = inputs[1:N+1]\n",
      "    Q = inputs[N+1]\n",
      "    queries = []\n",
      "    idx = N+2\n",
      "    for _ in range(Q):\n",
      "        i, j = inputs[idx], inputs[idx+1]\n",
      "        queries.append((i-1, j))\n",
      "        idx += 2\n",
      "\n",
      "    # Prepare sorted list with (value, original_index)\n",
      "    arr = sorted([(val, idx) for idx, val in enumerate(a)])\n",
      "    sorted_a = [val for val, idx in arr]\n",
      "    sorted_idx = [idx for val, idx in arr]\n",
      "\n",
      "    # Map from original index to position in sorted array\n",
      "    pos_in_sorted = [0]*N\n",
      "    for pos, idx in enumerate(sorted_idx):\n",
      "        pos_in_sorted[idx] = pos\n",
      "\n",
      "    # Prefix sums for sorted_a\n",
      "    prefix = [0]*(N+1)\n",
      "    for i in range(N):\n",
      "        prefix[i+1] = prefix[i] + sorted_a[i]\n",
      "\n",
      "    # Initial T\n",
      "    T0 = 0\n",
      "    for i in range(N):\n",
      "        T0 += (i+1)*sorted_a[i]\n",
      "\n",
      "    results = []\n",
      "\n",
      "    for i, j in queries:\n",
      "        old_val = a[i]\n",
      "        old_pos = pos_in_sorted[i]\n",
      "\n",
      "        # Remove old_val from sorted_a\n",
      "        # Find where j would go\n",
      "        # Remove old_val at old_pos, insert j at new_pos\n",
      "        # Compute new T\n",
      "\n",
      "        # Remove old_val\n",
      "        # The array without old_val is sorted_a[:old_pos] + sorted_a[old_pos+1:]\n",
      "        # Insert j\n",
      "        # Find new position for j\n",
      "        # Use bisect to find insertion point in the array without old_val\n",
      "\n",
      "        # Build the array without old_val\n",
      "        # For bisect, we need the array without old_val\n",
      "        arr_wo = sorted_a[:old_pos] + sorted_a[old_pos+1:]\n",
      "        new_pos = bisect.bisect_left(arr_wo, j)\n",
      "\n",
      "        # Now, compute new T\n",
      "        # The new array is:\n",
      "        # arr_wo[:new_pos] + [j] + arr_wo[new_pos:]\n",
      "        # Their multipliers are 1..N\n",
      "\n",
      "        # Compute prefix sums for arr_wo\n",
      "        prefix_wo = [0]*(N)\n",
      "        for k in range(N-1):\n",
      "            prefix_wo[k+1] = prefix_wo[k] + arr_wo[k]\n",
      "\n",
      "        # T = sum_{k=0}^{new_pos-1} (k+1)*arr_wo[k] + (new_pos+1)*j + sum_{k=new_pos}^{N-2} (k+2)*arr_wo[k]\n",
      "        T = 0\n",
      "        # Left part\n",
      "        T += prefix_wo[new_pos]\n",
      "        # Middle\n",
      "        T += (new_pos+1)*j\n",
      "        # Right part\n",
      "        if new_pos < N-1:\n",
      "            # sum_{k=new_pos}^{N-2} (k+2)*arr_wo[k]\n",
      "            # = sum_{k=new_pos}^{N-2} (k+1)*arr_wo[k] + sum_{k=new_pos}^{N-2} arr_wo[k]\n",
      "            T += (prefix_wo[N-1] - prefix_wo[new_pos])  # (k+1)*arr_wo[k]\n",
      "            T += (prefix_wo[N-1] - prefix_wo[new_pos])  # arr_wo[k]\n",
      "            # But this is double, so we need to add (k+2)*arr_wo[k]\n",
      "            # Actually, (k+2)*arr_wo[k] = (k+1)*arr_wo[k] + arr_wo[k]\n",
      "            # So, the above is correct\n",
      "\n",
      "        results.append(T)\n",
      "\n",
      "    return results\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00824           1443               1512          2955\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: milk_problem_solution.md -->\n",
      "To address the problem efficiently, the following reasoning and approach are adopted:\n",
      "\n",
      "### Problem Analysis\n",
      "\n",
      "- The total milk $T$ is maximized by assigning the largest multipliers (i.e., the largest $k$ in $k \\cdot a$) to the largest $a$ values. Thus, the optimal order is to sort $a$ in increasing order and assign the smallest multiplier to the smallest $a$.\n",
      "- For each query, we are to compute the new $T$ if $a_i$ is set to $j$, and then the cows are optimally reordered.\n",
      "\n",
      "### Efficient Approach\n",
      "\n",
      "1. **Initial Sorting and Preprocessing:**\n",
      "   - Sort the array $a$ and keep track of the original indices.\n",
      "   - Compute the initial $T$ using the sorted array.\n",
      "2. **For Each Query:**\n",
      "   - Remove $a_i$ from the sorted array and insert $j$.\n",
      "   - Find the new position of $j$ in the sorted array.\n",
      "   - Compute the new $T$ efficiently by adjusting only the affected positions.\n",
      "   - To avoid re-sorting for each query, use binary search and prefix sums.\n",
      "\n",
      "### Implementation Details\n",
      "\n",
      "- Use a list of tuples to keep track of values and their original indices.\n",
      "- Use prefix sums to quickly compute the sum of $a$ values up to a certain position.\n",
      "- For each query, use binary search to find where $j$ would be inserted.\n",
      "- Adjust the prefix sums and $T$ accordingly.\n",
      "\n",
      "### Code\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    import bisect\n",
      "\n",
      "    N = inputs[0]\n",
      "    a = inputs[1:N+1]\n",
      "    Q = inputs[N+1]\n",
      "    queries = []\n",
      "    idx = N+2\n",
      "    for _ in range(Q):\n",
      "        i, j = inputs[idx], inputs[idx+1]\n",
      "        queries.append((i-1, j))\n",
      "        idx += 2\n",
      "\n",
      "    # Prepare sorted list with (value, original_index)\n",
      "    arr = sorted([(val, idx) for idx, val in enumerate(a)])\n",
      "    sorted_a = [val for val, idx in arr]\n",
      "    sorted_idx = [idx for val, idx in arr]\n",
      "\n",
      "    # Map from original index to position in sorted array\n",
      "    pos_in_sorted = [0]*N\n",
      "    for pos, idx in enumerate(sorted_idx):\n",
      "        pos_in_sorted[idx] = pos\n",
      "\n",
      "    # Prefix sums for sorted_a\n",
      "    prefix = [0]*(N+1)\n",
      "    for i in range(N):\n",
      "        prefix[i+1] = prefix[i] + sorted_a[i]\n",
      "\n",
      "    # Initial T\n",
      "    T0 = 0\n",
      "    for i in range(N):\n",
      "        T0 += (i+1)*sorted_a[i]\n",
      "\n",
      "    results = []\n",
      "\n",
      "    for i, j in queries:\n",
      "        old_val = a[i]\n",
      "        old_pos = pos_in_sorted[i]\n",
      "\n",
      "        # Remove old_val from sorted_a\n",
      "        # Find where j would go\n",
      "        # Remove old_val at old_pos, insert j at new_pos\n",
      "        # Compute new T\n",
      "\n",
      "        # Remove old_val\n",
      "        # The array without old_val is sorted_a[:old_pos] + sorted_a[old_pos+1:]\n",
      "        # Insert j\n",
      "        # Find new position for j\n",
      "        # Use bisect to find insertion point in the array without old_val\n",
      "\n",
      "        # Build the array without old_val\n",
      "        # For bisect, we need the array without old_val\n",
      "        arr_wo = sorted_a[:old_pos] + sorted_a[old_pos+1:]\n",
      "        new_pos = bisect.bisect_left(arr_wo, j)\n",
      "\n",
      "        # Now, compute new T\n",
      "        # The new array is:\n",
      "        # arr_wo[:new_pos] + [j] + arr_wo[new_pos:]\n",
      "        # Their multipliers are 1..N\n",
      "\n",
      "        # Compute prefix sums for arr_wo\n",
      "        prefix_wo = [0]*(N)\n",
      "        for k in range(N-1):\n",
      "            prefix_wo[k+1] = prefix_wo[k] + arr_wo[k]\n",
      "\n",
      "        # T = sum_{k=0}^{new_pos-1} (k+1)*arr_wo[k] + (new_pos+1)*j + sum_{k=new_pos}^{N-2} (k+2)*arr_wo[k]\n",
      "        T = 0\n",
      "        # Left part\n",
      "        T += prefix_wo[new_pos]\n",
      "        # Middle\n",
      "        T += (new_pos+1)*j\n",
      "        # Right part\n",
      "        if new_pos < N-1:\n",
      "            # sum_{k=new_pos}^{N-2} (k+2)*arr_wo[k]\n",
      "            # = sum_{k=new_pos}^{N-2} (k+1)*arr_wo[k] + sum_{k=new_pos}^{N-2} arr_wo[k]\n",
      "            T += (prefix_wo[N-1] - prefix_wo[new_pos])  # (k+1)*arr_wo[k]\n",
      "            T += (prefix_wo[N-1] - prefix_wo[new_pos])  # arr_wo[k]\n",
      "            # But this is double, so we need to add (k+2)*arr_wo[k]\n",
      "            # Actually, (k+2)*arr_wo[k] = (k+1)*arr_wo[k] + arr_wo[k]\n",
      "            # So, the above is correct\n",
      "\n",
      "        results.append(T)\n",
      "\n",
      "    return results\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/milk_problem_solution.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00827           3978                 39          4017\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00703           3509                  1          3510\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00826800 |          3978 |                39 |         4017 |\n",
      "| researcher                    | $0.01363400 |          2285 |              1133 |         3418 |\n",
      "| researcher response formatter | $0.00824010 |          1443 |              1512 |         2955 |\n",
      "| terminator                    | $0.00702600 |          3509 |                 1 |         3510 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.03716810 |         11215 |              2685 |        13900 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250609_164136.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250609_164136.json\n",
      "\n",
      "Task took 49.8795 seconds\n",
      "Executing code for TEST CASE 1/10, on problem 3/307\n",
      "\n",
      "Executing code for TEST CASE 2/10, on problem 3/307\n",
      "\n",
      "Executing code for TEST CASE 3/10, on problem 3/307\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 1326_silver_milk_sum ============\n",
      "\n",
      "Total test cases: 3\n",
      "Correctly guessed test_cases: 0\n",
      "Accuracy: 0.00%\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 4/307: 1327_silver_field_day\n",
      "===============================================================\n",
      "⚠️ Skipping problem 1327_silver_field_day due to non-numeric data in test case 1: invalid literal for int() with base 10: 'GHGGH'\n",
      "No test cases found for 1327_silver_field_day\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 5/307: 1328_silver_pareidolia\n",
      "===============================================================\n",
      "⚠️ Skipping problem 1328_silver_pareidolia due to non-numeric data in test case 1: invalid literal for int() with base 10: 'bessiebessie'\n",
      "No test cases found for 1328_silver_pareidolia\n",
      "===============================================================\n",
      "\t\tEvaluating CMBAgent on problem 6/307: 1325_bronze_rotate_and_shift\n",
      "===============================================================\n",
      "Task:\n",
      "Write a function which solves the following problem: \n",
      "**Note: The time limit for this problem is 4s, 2x the default.**\n",
      "To celebrate the start of spring, Farmer John's $N$ cows ($1 \\leq N \\leq 2 \\cdot 10^5$) have invented an intriguing new dance, where they stand in a circle and re-order themselves in a predictable way.\n",
      "\n",
      "Specifically, there are $N$ positions around the circle, numbered sequentially from $0$ to $N-1$, with position $0$ following position $N-1$.  A cow resides at each position.  The cows are also numbered sequentially from $0$ to $N-1$.  Initially, cow $i$ starts in position $i$.  You are told a set of $K$ positions $0=A_1<A_2< \\ldots< A_K<N$ that are \"active\", meaning the cows in these positions are the next to move ($1 \\leq K \\leq N$).  \n",
      "\n",
      "In each minute of the dance, two things happen.  First, the cows in the active positions rotate: the cow at position $A_1$ moves to position $A_2$, the cow at position $A_2$ moves to position $A_3$, and so on, with the cow at position $A_K$ moving to position $A_1$.  All of these $K$ moves happen simultaneously, so the after the rotation is complete, all of the active positions still contain exactly one cow.  Next, the active positions themselves shift:\n",
      "$A_1$ becomes $A_1+1$, $A_2$ becomes $A_2+1$, and so on (if $A_i = N-1$ for some active position, then $A_i$ circles back around to $0$).\n",
      "\n",
      "Please calculate the order of the cows after $T$ minutes of the dance ($1\\le T\\le 10^9$).\n",
      "\n",
      "INPUT FORMAT (input arrives from the terminal / stdin):\n",
      "The first line contains three integers $N$, $K$, and $T$.\n",
      "\n",
      "The second line contains $K$ integers representing the initial set of active positions\n",
      "$A_1,A_2, \\ldots A_K$.  Recall that $A_1 = 0$ and that these are given in increasing order.\n",
      "\n",
      "OUTPUT FORMAT (print output to the terminal / stdout):\n",
      "Output the order of the cows after $T$ minutes, starting with the cow in position $0$, separated by\n",
      "spaces.\n",
      "\n",
      "SAMPLE INPUT:\n",
      "5 3 4\n",
      "0 2 3\n",
      "SAMPLE OUTPUT: \n",
      "1 2 3 4 0\n",
      "\n",
      "For the example above, here are the cow orders and $A$ for the first four\n",
      "timesteps:\n",
      "\n",
      "\n",
      "Initial, T = 0: order = [0 1 2 3 4], A = [0 2 3]\n",
      "T = 1: order = [3 1 0 2 4]\n",
      "T = 1: A = [1 3 4]\n",
      "T = 2: order = [3 4 0 1 2]\n",
      "T = 2: A = [2 4 0]\n",
      "T = 3: order = [2 4 3 1 0]\n",
      "T = 3: A = [3 0 1]\n",
      "T = 4: order = [1 2 3 4 0]\n",
      "\n",
      "SCORING:\n",
      "Inputs 2-7: $N \\leq 1000, T \\leq 10000$Inputs 8-13: No additional constraints.\n",
      "\n",
      "\n",
      "Problem credits: Claire Zhang\n",
      "\n",
      "Always name the primary function main_function, even if it requires helper functions\n",
      "Example:\n",
      "Function Input: [5, 3, 4, 0, 2, 3]\n",
      "Expected Function Output: [1, 2, 3, 4, 0]\n",
      "The function has to deliver the result strictly as a Python list:\n",
      "\t- If the result is a single integer n, return it as [n]\n",
      "\t- If the result is a list, return it as [a, b, c, ...]\n",
      "Do not include any extra text, explanation, or formatting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling researcher...\n",
      "\u001b[0m\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 researcher 0.01235           2079               1024          3103\n",
      "### Problem Analysis and Reasoning\n",
      "\n",
      "The problem describes a process where, at each minute, a set of \"active\" positions in a circle of $N$ cows is rotated, and then the set of active positions is shifted forward by one (wrapping around). The process is repeated $T$ times, and the goal is to determine the final order of cows.\n",
      "\n",
      "#### Key Observations\n",
      "\n",
      "1. **Active Positions Cycle**: The set of active positions cycles every $N$ steps, since after $N$ shifts, each $A_i$ returns to its original value modulo $N$.\n",
      "2. **Rotation is a Permutation**: Each minute, the cows at the active positions are permuted in a cycle.\n",
      "3. **Large $T$**: Since $T$ can be up to $10^9$, simulating each step is infeasible. Instead, the process must be modeled as a permutation applied repeatedly.\n",
      "\n",
      "#### Efficient Approach\n",
      "\n",
      "- For each position, determine where the cow originally at that position ends up after $T$ steps.\n",
      "- The process can be modeled as a permutation of the cows, and the permutation can be exponentiated (using binary exponentiation) to apply it $T$ times efficiently.\n",
      "\n",
      "#### Steps\n",
      "\n",
      "1. For each position, determine where the cow at that position moves after one step.\n",
      "2. Build the permutation array `P` such that `P[i]` is the new position of the cow currently at position `i` after one step.\n",
      "3. Use permutation exponentiation to compute the effect of applying `P` $T$ times.\n",
      "4. Apply the resulting permutation to the initial order.\n",
      "\n",
      "### Implementation\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    N, K, T = inputs[0], inputs[1], inputs[2]\n",
      "    A = inputs[3:3+K]\n",
      "    # Step 1: For each position, determine where the cow at that position moves after one step\n",
      "    # Build the permutation P: P[i] = new position of cow at i after one step\n",
      "    P = [i for i in range(N)]\n",
      "    # For each minute, the cows at A are rotated, and A is shifted\n",
      "    # Let's build the mapping for one step\n",
      "    # For each position, find if it's in A, and if so, where it moves\n",
      "    pos_in_A = [-1]*N\n",
      "    for idx, pos in enumerate(A):\n",
      "        pos_in_A[pos] = idx\n",
      "    # After one step, A becomes [(a+1)%N for a in A]\n",
      "    # The cow at A[i] moves to A[(i+1)%K]\n",
      "    # So, for each A[i], the cow at A[i] moves to (A[(i+1)%K]+1)%N\n",
      "    # But since A is shifted after the rotation, the mapping is:\n",
      "    # - The cow at A[i] moves to (A[(i+1)%K]+1)%N\n",
      "    # For all other positions, the cow stays in place (but their index increases by 1 mod N)\n",
      "    # Let's build the mapping for all positions\n",
      "    # First, build the set of next active positions\n",
      "    next_A = [(a+1)%N for a in A]\n",
      "    # Build a mapping from current active position to next active position\n",
      "    for i in range(K):\n",
      "        cur = A[i]\n",
      "        nxt = next_A[(i+1)%K]\n",
      "        P[cur] = nxt\n",
      "    # For all other positions, after one step, their index increases by 1 mod N if not in A\n",
      "    # But actually, only the active positions are changed, others stay in place\n",
      "    # So, for all positions not in A, they stay in place\n",
      "    # Now, we have the permutation P: P[i] = new position of cow at i after one step\n",
      "    # But we need the inverse: for each position, from where does the cow come after one step?\n",
      "    # Let's build the inverse permutation Q: Q[i] = position where the cow at i comes from\n",
      "    Q = [i for i in range(N)]\n",
      "    for i in range(N):\n",
      "        Q[P[i]] = i\n",
      "    # Now, to apply the permutation T times, we can use binary exponentiation\n",
      "    def permute_pow(Q, T):\n",
      "        N = len(Q)\n",
      "        res = [i for i in range(N)]\n",
      "        base = Q[:]\n",
      "        while T:\n",
      "            if T % 2:\n",
      "                res = [base[res[i]] for i in range(N)]\n",
      "            base = [base[base[i]] for i in range(N)]\n",
      "            T //= 2\n",
      "        return res\n",
      "    final_perm = permute_pow(Q, T)\n",
      "    # The cow at position i after T steps is the cow that started at final_perm[i]\n",
      "    result = [final_perm[i] for i in range(N)]\n",
      "    return result\n",
      "</code>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Formatting...\n",
      "\n",
      "             Model                         agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "o3-mini-2025-01-31 researcher_response_formatter 0.00721           1334               1306          2640\n",
      "**Markdown:**\n",
      "\n",
      "```markdown\n",
      "<!-- filename: permutation_rotation_analysis.md -->\n",
      "### Problem Analysis and Reasoning\n",
      "\n",
      "The problem describes a process where, at each minute, a set of \"active\" positions in a circle of $N$ cows is rotated, and then the set of active positions is shifted forward by one (wrapping around). The process is repeated $T$ times, and the goal is to determine the final order of cows.\n",
      "\n",
      "#### Key Observations\n",
      "\n",
      "1. **Active Positions Cycle**: The set of active positions cycles every $N$ steps, since after $N$ shifts, each $A_i$ returns to its original value modulo $N$.\n",
      "2. **Rotation is a Permutation**: Each minute, the cows at the active positions are permuted in a cycle.\n",
      "3. **Large $T$**: Since $T$ can be up to $10^9$, simulating each step is infeasible. Instead, the process must be modeled as a permutation applied repeatedly.\n",
      "\n",
      "#### Efficient Approach\n",
      "\n",
      "- For each position, determine where the cow originally at that position ends up after $T$ steps.\n",
      "- The process can be modeled as a permutation of the cows, and the permutation can be exponentiated (using binary exponentiation) to apply it $T$ times efficiently.\n",
      "\n",
      "#### Steps\n",
      "\n",
      "1. For each position, determine where the cow at that position moves after one step.\n",
      "2. Build the permutation array `P` such that `P[i]` is the new position of the cow currently at position `i` after one step.\n",
      "3. Use permutation exponentiation to compute the effect of applying `P` $T$ times.\n",
      "4. Apply the resulting permutation to the initial order.\n",
      "\n",
      "### Implementation\n",
      "\n",
      "<code>\n",
      "def main_function(inputs):\n",
      "    N, K, T = inputs[0], inputs[1], inputs[2]\n",
      "    A = inputs[3:3+K]\n",
      "    # Step 1: For each position, determine where the cow at that position moves after one step\n",
      "    # Build the permutation P: P[i] = new position of cow at i after one step\n",
      "    P = [i for i in range(N)]\n",
      "    # For each minute, the cows at A are rotated, and A is shifted\n",
      "    # Let's build the mapping for one step\n",
      "    # For each position, find if it's in A, and if so, where it moves\n",
      "    pos_in_A = [-1]*N\n",
      "    for idx, pos in enumerate(A):\n",
      "        pos_in_A[pos] = idx\n",
      "    # After one step, A becomes [(a+1)%N for a in A]\n",
      "    # The cow at A[i] moves to A[(i+1)%K]\n",
      "    # So, for each A[i], the cow at A[i] moves to (A[(i+1)%K]+1)%N\n",
      "    # But since A is shifted after the rotation, the mapping is:\n",
      "    # - The cow at A[i] moves to (A[(i+1)%K]+1)%N\n",
      "    # For all other positions, the cow stays in place (but their index increases by 1 mod N)\n",
      "    # Let's build the mapping for all positions\n",
      "    # First, build the set of next active positions\n",
      "    next_A = [(a+1)%N for a in A]\n",
      "    # Build a mapping from current active position to next active position\n",
      "    for i in range(K):\n",
      "        cur = A[i]\n",
      "        nxt = next_A[(i+1)%K]\n",
      "        P[cur] = nxt\n",
      "    # For all other positions, after one step, their index increases by 1 mod N if not in A\n",
      "    # But actually, only the active positions are changed, others stay in place\n",
      "    # So, for all positions not in A, they stay in place\n",
      "    # Now, we have the permutation P: P[i] = new position of cow at i after one step\n",
      "    # But we need the inverse: for each position, from where does the cow come after one step?\n",
      "    # Let's build the inverse permutation Q: Q[i] = position where the cow at i comes from\n",
      "    Q = [i for i in range(N)]\n",
      "    for i in range(N):\n",
      "        Q[P[i]] = i\n",
      "    # Now, to apply the permutation T times, we can use binary exponentiation\n",
      "    def permute_pow(Q, T):\n",
      "        N = len(Q)\n",
      "        res = [i for i in range(N)]\n",
      "        base = Q[:]\n",
      "        while T:\n",
      "            if T % 2:\n",
      "                res = [base[res[i]] for i in range(N)]\n",
      "            base = [base[base[i]] for i in range(N)]\n",
      "            T //= 2\n",
      "        return res\n",
      "    final_perm = permute_pow(Q, T)\n",
      "    # The cow at position i after T steps is the cow that started at final_perm[i]\n",
      "    result = [final_perm[i] for i in range(N)]\n",
      "    return result\n",
      "</code>\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving report...\n",
      "\n",
      "Execution results:\n",
      "\n",
      "Execution output: Content saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/permutation_rotation_analysis.md\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Calling control...\n",
      "\u001b[0m\n",
      "             Model   agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 control 0.00763           3657                 39          3696\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Step number:** 1 out of 1.\n",
      " \n",
      "**Sub-task:** solve the main task.\n",
      " \n",
      "**Agent in charge of sub-task:** `researcher`\n",
      " \n",
      "**Instructions:**\n",
      " \n",
      "solve the main task.\n",
      " \n",
      "**Status:** completed ✅\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Terminating...\n",
      "\n",
      "             Model      agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "gpt-4.1-2025-04-14 terminator 0.00638           3188                  1          3189\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Session terminated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Displaying cost…\n",
      "\n",
      "| Agent                         | Cost ($)    | Prompt Tokens | Completion Tokens | Total Tokens |\n",
      "|:------------------------------|------------:|--------------:|------------------:|-------------:|\n",
      "| control                       | $0.00762600 |          3657 |                39 |         3696 |\n",
      "| researcher                    | $0.01235000 |          2079 |              1024 |         3103 |\n",
      "| researcher response formatter | $0.00721380 |          1334 |              1306 |         2640 |\n",
      "| terminator                    | $0.00638400 |          3188 |                 1 |         3189 |\n",
      "|-------------------------------|-------------|---------------|-------------------|--------------|\n",
      "| Total                         | $0.03357380 |         10258 |              2370 |        12628 |\n",
      "\n",
      "Cost report data saved to: /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/cost_report_20250609_164230.json\n",
      "\n",
      "\n",
      "Timing report saved to /mnt/p/stage/cmbagent_benchmark/python/evaluation/cmbagent_output/timing_report_20250609_164230.json\n",
      "\n",
      "Task took 46.7382 seconds\n",
      "Executing code for TEST CASE 1/12, on problem 6/307\n",
      "\n",
      "Executing code for TEST CASE 2/12, on problem 6/307\n",
      "\n",
      "Executing code for TEST CASE 3/12, on problem 6/307\n",
      "\n",
      "\n",
      " =========== BENCHMARK RESULT FOR CMBAGENT ON PROBLEM 1325_bronze_rotate_and_shift ============\n",
      "\n",
      "Total test cases: 3\n",
      "Correctly guessed test_cases: 0\n",
      "Accuracy: 0.00%\n",
      "⚠️ Skipped problems due to non-numeric data:\n",
      "- 1330_gold_pareidolia\n",
      "- 1327_silver_field_day\n",
      "- 1328_silver_pareidolia\n",
      "\n",
      "=== Total aggregated cost across all problems ===\n",
      "|    | Agent                         |   Cost ($) |   Prompt Tokens |   Completion Tokens |   Total Tokens |\n",
      "|----|-------------------------------|------------|-----------------|---------------------|----------------|\n",
      "|  0 | control                       |  0.021612  |           10338 |                 117 |          10455 |\n",
      "|  1 | researcher                    |  0.034506  |            6413 |                2710 |           9123 |\n",
      "|  2 | researcher response formatter |  0.0202268 |            3640 |                3687 |           7327 |\n",
      "|  3 | terminator                    |  0.017886  |            8931 |                   3 |           8934 |\n",
      "|  4 | Total                         |  0.0942308 |           29322 |                6517 |          35839 |\n",
      "\n",
      "============ BENCHMARK SUMMARY FOR CMBAGENT FOR ALL PROBLEMS ==============\n",
      "1333_platinum_good_bitstrings:\n",
      "  Total test cases: 3\n",
      "  Correctly solved: 0\n",
      "  Accuracy: 0.00%\n",
      "\n",
      "1326_silver_milk_sum:\n",
      "  Total test cases: 3\n",
      "  Correctly solved: 0\n",
      "  Accuracy: 0.00%\n",
      "\n",
      "1325_bronze_rotate_and_shift:\n",
      "  Total test cases: 3\n",
      "  Correctly solved: 0\n",
      "  Accuracy: 0.00%\n",
      "\n",
      "============ CONCLUSION ===============\n",
      "Average accuracy over all problems: 0.00%\n",
      "=======================================\n",
      "\n",
      "Benchmark evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "# benchmark on real usaco problems (right now manually limited to 5 problems and 3 test cases each for time efficient reasons)\n",
    "# see the end for benchmark conclusion\n",
    "\n",
    "run_benchmark(\"/mnt/p/stage/cmbagent_benchmark/data/clean/usaco_clean_307.json\",\n",
    "              \"/mnt/p/stage/cmbagent_benchmark/data/clean/usaco_tests\",\n",
    "              eval_cmbagent=True,\n",
    "              cmbagent_model=\"gpt-4o-mini\",\n",
    "              eval_normal_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841460f-8290-4910-8287-d96ba7762402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
